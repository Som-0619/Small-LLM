# Small LLM Fine-Tuning using DistilGPT-2

This project demonstrates how to fine-tune a lightweight Large Language Model (LLM) â€” specifically DistilGPT-2, a distilled and faster version of GPT-2 â€” on a custom dataset using Hugging Face Transformers.
Itâ€™s ideal for learning, experimentation, and small-scale language modeling tasks.

ğŸ§  Overview

This notebook walks through:

Setting up the environment

Loading and tokenizing your custom dataset

Fine-tuning a pre-trained small LLM (DistilGPT-2)

Saving and testing the fine-tuned model

Youâ€™ll end up with a custom-trained text generation model that learns patterns from your dataset and can generate similar outputs.

ğŸ“¦ Requirements

Install all dependencies before running the notebook:

!pip install transformers datasets torch accelerate


âœ… Recommended: Run this on Google Colab with a GPU runtime enabled (Runtime â†’ Change runtime type â†’ GPU).

âš™ï¸ Project Structure
Cell	Description
Cell 1	Install required libraries
Cell 2	Import all dependencies
Cell 3	Load dataset (custom text file or Hugging Face dataset)
Cell 4	Preprocess and tokenize data
Cell 5	Prepare data for model input
Cell 6	Load DistilGPT-2 model and tokenizer
Cell 7	Define training parameters using TrainingArguments
Cell 8	Initialize the Trainer
Cell 9	Train and save the model (fine-tuning step)
Cell 10	Load the fine-tuned model and test it with sample prompts
ğŸ§° Model Used

DistilGPT-2

A distilled (compressed) version of OpenAIâ€™s GPT-2

Has ~82M parameters (smaller and faster than GPT-2â€™s 124M)

Great for low-resource environments and quick fine-tuning

ğŸ§ª Testing the Model

Once fine-tuning is complete, you can test your model like this:

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "./finetuned-llm-distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

ğŸ•’ Training Time

Training time depends on:

Dataset size

Number of epochs

GPU availability

For small datasets (~few KB), fine-tuning usually takes 3â€“10 minutes on a Colab GPU.

ğŸ’¾ Saving & Loading

The model and tokenizer are saved locally at:

./finetuned-llm-distilgpt2/


You can reload them anytime for inference or further fine-tuning.

ğŸ” Example Use Cases

Text generation or story completion

Chatbot response modeling

Domain-specific writing (e.g., law, medicine, tech blogs)

Prototype-level LLM customization

ğŸ“š References

Hugging Face Transformers Documentation

DistilGPT-2 Model Card

Google Colab Guide
